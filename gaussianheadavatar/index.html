<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Gaussian Head Avatar's Project Page</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2 class="text-center">Gaussian Head Avatar: </h2>
            <h2 class="text-center">Ultra High-fidelity Head Avatar via Dynamic Gaussians</h2>
            <h4 style="color:#5a6268;">CVPR 2024</h4>
            <hr>
            <h6 class="text-center">Yuelang Xu<sup>1</sup>, Benwang Chen<sup>1</sup>, <a href="https://lizhe00.github.io/">Zhe Li<sup>1</sup></a>, <a href="https://hongwenzhang.github.io/">Hongwen Zhang<sup>1</sup></a>, <a href="https://lizhenwangt.github.io/">Lizhen Wang<sup>1</sup></a>, Zerong Zheng<sup>2</sup>, <a href="http://www.liuyebin.com/">Yebin Liu<sup>1</sup></a></h6>
            <p class="text-center"><sup>1</sup>Tsinghua University  <sup>2</sup>NNKosmos Technology </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2312.03029" role="button"  target="_blank">
                  <i class="fa fa-file"></i> ArXiv</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="assets/Gaussian_Head_Avatar.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.youtube.com/watch?v=kvrrI3EoM5g" role="button"  target="_blank">
                  <i class="fa fa-youtube"></i> Video</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/YuelangX/Gaussian-Head-Avatar" role="button"  target="_blank">
                    <i class="fa fa-github"></i> Code</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
              <!-- <br><br> -->
          <p class="text-left"> Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups.
            In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling.
            We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. 
            The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. 
            Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure.
            Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.
            </p>
            <p class="text-left">&nbsp;</p>
            <img src="assets/teaser.png" width="1080" alt="">
            <p class="text-left">Fig 1.Gaussian head avatar achieves ultra high-fidelity image synthesis with controllable expressions at 2K resolution. The above shows different views of the synthesized avatar, and the bottom shows different identities animated by the same expression. 16 views are used during the training.</p>
            <p>&nbsp;</p>
            <hr>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
          <div class="col-12 text-center">
            <h3>Method</h3>
            <img src="assets/overview.png" width="1080" alt="">
            <p class="text-left">Fig 2.&nbsp;Overview of the Gaussian Head Avatar.
              The overview of the Gaussian Head Avatar rendering and reconstruction. We first optimize the guidance model including a neutral mesh, a deformation MLP and a color MLP in the Initialization stage. Then we use them to initialize the neutral Gaussians and the dynamic generator. Finally, 2K RGB images are synthesized through differentiable rendering and the super-resolution network. The Gaussian Head Avatar are trained under the supervision of multi-view RGB videos.</p>
            <p>&nbsp;</p>
            <hr>
          </div>
        </div>

        <div class="row">
            <div class="col-12 text-center">
              <h3>Results</h3>
              <p>&nbsp;</p>
              <video width="1080" height=""  muted autoplay="autoplay" loop="loop">
                <source src="assets/more_cross_1.mp4" type="video/mp4">
              </video>
            </div>
          </div>

        <div class="row">
            <div class="col-12 text-center">
              <video width="1080" height=""  muted autoplay="autoplay" loop="loop">
                <source src="assets/more_cross_2.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          
        <div class="row">
            <div class="col-12 text-center">
              <h3>Results</h3>
              <video width="1080" height=""  muted autoplay="autoplay" loop="loop">
                <source src="assets/more_cross_3.mp4" type="video/mp4">
              </video>
              <p class="text-left">Fig 3.&nbsp;Cross-identity reenactment results.</p>
              <p>&nbsp;</p>
            </div>
          </div>


        <div class="row">
            <div class="col-12 text-center">
              <p>&nbsp;</p>
              <video width="1080" height=""  muted autoplay="autoplay" loop="loop">
                <source src="assets/8_views_input.mp4" type="video/mp4">
              </video>
              <p class="text-left">Fig 4.&nbsp;8 views input.</p>
              <p>&nbsp;</p>
            </div>
          </div>

        <div class="row">
            <div class="col-12 text-center">
              <p>&nbsp;</p>
              <video width="1080" height=""  muted autoplay="autoplay" loop="loop">
                <source src="assets/6_views_input.mp4" type="video/mp4">
              </video>
              <p class="text-left">Fig 5.&nbsp;6 views input.</p>
              <p>&nbsp;</p>
            </div>
          </div>

        <div class="row">
            <div class="col-12 text-center">
              <p>&nbsp;</p>
              <img src="assets/comparisons_self.jpg" width="1080" alt="">
              <p class="text-left">Fig 6.&nbsp;Qualitative comparisons of different methods on self reenactment task. From left to right: NeRFBlendShape, NeRFace, HAvatar and Ours. Our method can reconstruct details like beards, teeth, etc. with high quality.</p>
              <p>&nbsp;</p>
            </div>
          </div>

        <div class="row">
            <div class="col-12 text-center">
              <p>&nbsp;</p>
              <img src="assets/comparisons_cross.jpg" width="1080" alt="">
              <p class="text-left">Fig 7.&nbsp;Qualitative comparisons of different methods on cross-identity reenactment task. From left to right: NeRFBlendShape, NeRFace, HAvatar and Ours. Our method synthesizes high-fidelity images while ensuring the accuracy of expression transfer.</p>
              <p>&nbsp;</p>
              <hr>
            </div>
          </div>


      </div>
  </section>

  <!-- overview video -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Demo Video</h3>
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/kvrrI3EoM5g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </div>
            <hr>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
              <code>@inproceedings{xu2023gaussianheadavatar,
                title={Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians},
                author={Xu, Yuelang and Chen, Benwang and Li, Zhe and Zhang, Hongwen and Wang, Lizhen and Zheng, Zerong and Liu, Yebin},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                year={2024}
              }</code></pre>
          <hr>
      </div>
    </div>
  </div>


  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Reference</h3>
          <p class="text-left">
            NeRFBlendShape: <em>Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong,
            Yudong Guo, and Juyong Zhang. Reconstructing person-
            alized semantic facial nerf models from monocular video.
            ACM Transactions on Graphics (Proceedings of SIGGRAPH
            Asia), 41(6), 2022.</em>
          </p>
          <p class="text-left">
            NeRFace: <em>Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias
            Niessner. Dynamic neural radiance fields for monocular 4d
            facial avatar reconstruction. In Proceedings of the IEEE/CVF
            Conference on Computer Vision and Pattern Recognition
            (CVPR), pages 8645-8654, 2021.</em>
          </p>
          <p class="text-left">
            HAvatar: <em>Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen
            Zhang, Jinli Suo, and Yebin Liu. Havatar: High-fidelity head
            avatar via facial model conditioned neural radiance field.
            ACM Trans. Graph. 2023.</em>
          </p>
          <hr>
      </div>
    </div>
  </div>

</body>
</html>
